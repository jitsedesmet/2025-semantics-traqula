## Software Architecture
{:#architecture}

Parsers are typically implemented in one of three ways:

1. **Hand-built parsers**: These are manually implemented parsers tailored to a specific grammar.
While they can be highly performant due to low-level optimizations and language-specific design,
they are often difficult to maintain, extend, or modularize. 
2. **Parser generators**: Tools such as [ANTLR](cite:cites parr1995antlr) and [Bison](cite:cites bison-gnu) use a Domain Specific Language (DSL),
typically based on Extended Backus–Naur Form (EBNF), to define a grammar.
These grammars are then compiled into standalone parser code.
While powerful, such approaches introduce a compile step and tend to be rigid, making modular extensions cumbersome. 
3. **Parser building toolkits**: Libraries such as [Chevrotain](cite:cites chevrotain) offer a hybrid approach,
enabling declarative grammar specification within a host programming language.
These toolkits eliminate the compile step and allow for flexible,
programmatic grammar definitions with fine-grained control over behavior and integration.

To support modularity while keeping the mental model approachable, a modular parser should be build using a parser building toolkit.
Parsing itself is typically divided into multiple phases [](cite:cites alfred2007compilers),
of which the following are relevant to this work:

1. **Lexical Analysis (*scanning*)**: A lexer transforms a character stream into a token stream. 
2. **Syntax Analysis (*parsing*)**: A parser transforms the token stream into an abstract syntax tree (AST). 
3. **Semantic Analysis**: Performed during or after parsing,
this phase validates constraints not enforced by grammar alone.
For instance, SPARQL forbids binding to a variable which is already in scope.

<!--
Traqula performs some semantic checks during parsing to generate precise and contextual error messages while token positions are still available,
enabling accurate diagnostics with row and column offsets.
-->

Inspired by the [Comunica modular query engine](cite:cites comunica) codebase,
the codebase of a modular parser should also follow a [monorepo](https://monorepo.tools/) structure, dividing its code into modular packages.
Within the Comunica codebase, the monorepo structure allows it to define many different builds (eg. a minimal built for the web, and a general built with and without file system access). Similar benefits can be expected in the adoption of a monorepo structure within the modular parser:

1. **Engines**: These are prebuilt, ready-to-use components such as SPARQL 1.1 and 1.2 parsers or generators. 
2. **Non-engine packages**: These expose modular building blocks used to construct engines,
such as grammar fragments or core construction utilities.

However, unlike Comunica which uses Components.js, a dependency injection framework using RDF based config files,
the modular query engine can be configured within the host language itself since components share a similar interface.
We propose that a parser be build using a builder pattern and that parser packages export the builder used, so other may extend upon it.
Using a builder pattern for the parser allows you to take a builder that is used to build one parser and manipulate the grammar rules to construct a new parser.

Concretely, we propose a builder which allows rules to be registered by name into a rule map,
thereby creating a loose coupling between registered rules.
Each rule is defined as a `ParserRule` object, containing both a rule name and a rule implementation.
Rule implementations can be expressed declaratively using Chevrotain’s grammar definition functions like:

1. **`SUBRULE`**: invokes another rule, registered under some name in the current parser,
2. **`MANY`**: matches zero or more occurrences of a pattern,
3. **`OR`**: matches one of several alternatives.

We propose, each rule implementation returns a function that,
when invoked, receives the parsing context and any parameters, and outputs part of the final syntax tree.
[](#ruleDef) shows an example parser rule definition.
The ParserBuilder can then be used for compositional construction and extension through methods like `addRule`, `deleteRule`, `merge`, and typePatch.
The typePatch utility would enable type updates to existing rules
— particularly useful when extending or modifying a dependent rule without altering the original rule’s implementation.
After the construction of your parser, you can build it, as shown in [](#parserBuild),
returning a parser which allows you to start parsing a string from any of the parser rules added to the builder
- a property transferred from the underlying parser builder toolkit.

<figure id="ruleDef" class="listing">
````/code/ruleDef.ts````
<figcaption markdown="block">
The definition of a parser rule parsing either a URI of the nil token, returning the parser URI or null respectively.
</figcaption>
</figure>

<figure id="parserBuild" class="listing">
````/code/parserBuild.ts````
<figcaption markdown="block">
The construction of a parser including the iriOrNil rule constructed in [](#ruleDef).
It also shows how to parse using the iriOrNil rule as the starting rule.
</figcaption>
</figure>

As for the lexer, a similar approach to the parser should be taken.
Tokens should be coupled loosely through a name-definition map.
The consumption of a token then results in the consumption of the token with that name in the used lexer.
Besides that our only requirement is that the tokens can be expressed through the definition of a regex.
