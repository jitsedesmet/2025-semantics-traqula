## Software Architecture
{:#architecture}

A parser is typically written in one of three ways:
1. a hand-built parser - execution times very on the implementation.
Has potential to be very fast since a combination of low level optimizations and language specific choices can be made.
2. using a parser generator (eg. [ANTLR](cite:cites parr1995antlr), [Bison](cite:cites bison-gnu)) - creates an executable parser based on some specific format. The parsers are 'compiled' from some Domain Specific Language (DSL), which is typically Extended Backusâ€“Naur form (EBNF) based.
3. A Parser Building Toolkit (eg. [Chevrotain](cite:cites chevrotain)) - a hybrid of the previous two.
It allows you to declaratively define rules like using a parser generator without the compile step, while staying in the confinements of a programming language.

To foster modularity while keeping down mental overhead of the modularity, we chose to use a parser building toolkit.
Additionally, parsing typically happens in [multiple phases](cite:cites alfred2007compilers), relevant in this work are:
1. Lexical Analysis/ scanning - done by a lexer: transforms character stream into token stream.
2. Syntax analysis/ parsing - done by a parser: transforms token stream to syntax tree - creating a tree representation of the language.
3. Semantic Analysis - likely done by the parser, after parsing: Validates the syntax tree - take in the example of SPARQL: you may not bind to a variable already in scope.

In the case of Traqula, some semantic checks can be done during the parsing phase,
doing so creates clear error messages as the context of tokens is still present at parse time,
as such the exception can tell at what token (line and offset) an error occurred.   

The Traqula project is a [monorepo](https://monorepo.tools/) consisting of 2 types of packages:
1. engines: these are software packages ready to be used by some consumer, for example a prebuilt SPARQL parser
2. non-engines (packages): these are software packages that can be used to construct you own engines.

Traqula exports a number of engines like: parser for SPARQL 1.1 and 1.2, and generators for SPARQL 1.1 and 1.2.
These engines in turn rely on non-engine packages such as the Traqula core package and the SPARQL grammar rule packages.
The Traqula core package is where most of the magic happens in regard to the construction of lexers, parsers and generators.
Each of these software components has their own dedicated builder, the LexerBuilder, parserBuilder and GeneratorBuilder.

## Lexer Builder

Lexers can be build using the LexerBuilder.
A lexer consists of a list of token definitions that are tried in order.
The first token definition matching to the current location of charter stream gets emitted, and the character stream gets consumed for a non-zero number of character matching the token definition.
It is thus essential that a token definition that is a subset of another is located before the other definition.
Take for example _tokenA_ defined by the regex `/appel/` should appear in the list before _tokenB_ defined by `/a[a-z]*/`. 

A token is defined using a regex like:
<figure id="token-def" class="listing">
````/code/tokenDefinition.ts````
<figcaption markdown="block">
This is some nice code!
</figcaption>
</figure>

And the LexerBuilder allows you to easily create that list in the correct order providing function like `merge`,
`moveBefore`, `moveAfter`, etc.
While it's type safety provides limited compile time checks on the validity of the lexer.

## Parser Builder

Parser are constructed using the ParserBuilder.
The parser consists of a map of grammar rules mapping rule names to rule implementations.
Rules are thus loosely coupled through their names.

Grammar Rules are defined by an object matching the `ParserRule` type, which is an object containing the name and rule implementation.
The rule implementation in tern has access to de declarative grammar definition function provided by Chevrotain like:
1. SUBRULE: continue parsing using a subrule, like a function call calling the function registered under some name in the current parser,
2. MANY: parsing the context zero or more times, and
3. OR: parsing one of the branches provided.
As well as a WeakMap cache that can be used to optimize consecutive parses,
for example when a rule constructs large arrays that can be cached between executions. 

Each rule should then return a function that can be executed by the parser when parsing,
this function gets a context entry and parameters if need be, and return a partial Syntax Tree. 
An example rule definition is:

<figure id="ruleDef" class="listing">
````/code/ruleDef.ts````
<figcaption markdown="block">
This is some nice code!
</figcaption>
</figure>


These rule definitions can then be used to construct a parser using the parser builder which provides methods like
`typePatch`, `addRule`, `deleteRule`, and `merge`.
After the construction of your parser, you can build it,
returning a parser which allows you to start parsing a string from any of the parser rules added to the builder.


<figure id="parserBuild" class="listing">
````/code/parserBuild.ts````
<figcaption markdown="block">
This is some nice code!
</figcaption>
</figure>


## Generator Builder

A generator can be used to convert a created AST to a string again.
This can be done in a similar way to the parser generator, namely an object with its name and definition.
The definition is again a function that receives declarative functions used for generation and return a function that can be called by the generator.
That function receives the AST, a context and parameter types.

